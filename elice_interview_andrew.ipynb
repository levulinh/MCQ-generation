{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elice Interview Mini-project\n",
    "### **Interviewee**: Vu Linh Le (Andrew)\n",
    "### **Position**: AI Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "The candidate is expected to develop a system that can automatically generate high-quality\n",
    "multiple-choice quizzes from textbooks or PDF documents. The quizzes should be relevant to\n",
    "the content of the given text and provide high-quality questions with multiple-choice answers.\n",
    "Define what \"good quality\" is for this project and outline your strategy to enhance model\n",
    "performance for better question quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary: Text extraction from PDF\n",
    "Assume that we will only work with PDF documents. In this step, the code will extract the text from the PDF documents. The text will be used as the input for the next steps.\n",
    "\n",
    "**Library used**: PyPDF2.\n",
    "I included a sample PDF file for testing purposes. The text is about my favorite Vietnamese noodle soup, Bun Bo Hue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "%conda env create -f environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import ipywidgets as widgets\n",
    "import json\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                page = reader.pages[page_num]\n",
    "                text += page.extract_text()\n",
    "\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note***: Even though the code extract irrelevant text, it will not be handled in this notebook for the sake of time and simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1: Using solely OpenAI's GPT-3.5 API\n",
    "The purpose of this solution is to demonstrate the use of OpenAI's GPT-3.5 API to generate multiple-choice questions from a given text. Skills shown in this solution include:\n",
    "- Understanding how to use third-party APIs to quickly prototype a solution\n",
    "- Prompt engineering to get the best results from the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class for the MCQ\n",
    "class MCQ:\n",
    "    def __init__(self, question, choices, answer, explanation):\n",
    "        self.question = question\n",
    "        self.choices = choices\n",
    "        self.answer = answer\n",
    "        self.explanation = explanation\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json(response):\n",
    "        json_str = response.choices[0].message.content\n",
    "        json_obj = json.loads(json_str)\n",
    "        try:\n",
    "            question = json_obj[\"question\"]\n",
    "            choices = json_obj[\"choices\"]\n",
    "            answer = json_obj[\"answer\"]\n",
    "            explanation = json_obj[\"explanation\"]\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing JSON\")\n",
    "            print(f\"Error: {e}\")\n",
    "            return None\n",
    "        return MCQ(question, choices, answer, explanation)\n",
    "\n",
    "    def display_mcq(self, output=None):\n",
    "        # Create a RadioButtons widget for the choices\n",
    "        # options = widgets.RadioButtons(options=self.choices, description=\"\", disabled=False)\n",
    "        CHOICE_LETTERS = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "        # Create a widget for displaying the question\n",
    "        question_widget = widgets.HTML(value=f\"<h3>{self.question}</h3>\")\n",
    "\n",
    "        choice_widgets = [widgets.HTML(value=self.choices[i]) for i in range(4)]\n",
    "        vbox = widgets.VBox([choice_widgets[0], choice_widgets[1], choice_widgets[2], choice_widgets[3]])\n",
    "        options = widgets.Select(options=CHOICE_LETTERS, description=\"Your answer\", disabled=False)\n",
    "        answer_button = widgets.Button(description=\"Answer\")\n",
    "        # Function to handle the submission of the answer\n",
    "        def on_answer_button_clicked(_):\n",
    "            # Get the selected answer\n",
    "            selected_answer = options.value\n",
    "            # Check if the selected answer is correct\n",
    "            if selected_answer == self.answer:\n",
    "                feedback_widget.value = \"<h3 style='color: green;'>Correct!</h3>\"\n",
    "            else:\n",
    "                feedback_widget.value = f\"<h3 style='color: red;'>Incorrect. The correct answer is {self.answer}</h3>\"\n",
    "            feedback_widget.value += f\"<p>{self.explanation}</p>\"\n",
    "\n",
    "        # Register the event handler for the button\n",
    "        answer_button.on_click(on_answer_button_clicked)\n",
    "\n",
    "        # Create a widget for displaying feedback\n",
    "        feedback_widget = widgets.HTML(value=\"\")\n",
    "\n",
    "        # Display the widgets\n",
    "        if output:\n",
    "            with output:\n",
    "                display(question_widget, vbox, options, answer_button, feedback_widget)\n",
    "        else:\n",
    "            display(question_widget, vbox, options, answer_button, feedback_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for the API useage, details in setting up the API is presented in the solution pdf file.\n",
    "import constants\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=constants.OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an English teacher creating a multiple-choice question based on a given passage. Vary the difficulty of the questions from very easy to very difficult. Provide the question, four choices labeled A, B, C, and D, and indicate the correct answer. Separate the elements in the response by a new line.\n",
    "You return the response following strictly the format provided in the example below.\n",
    "You give no narration.\n",
    "\n",
    "Example for user's input:\n",
    "Passage: \\{passage\\}\"|\"Difficulty: {difficulty}\n",
    "\n",
    "Example for response:\n",
    "\\n{\\\"question\\\": question Data, \\\"choices\\\":[choice A, choice B,\n",
    "choice C, choice D], \\\"answer\\\": \\\" A, B, C, or D \\\",\n",
    "\\\"explanation\\\": explaination data values}\n",
    "\"\"\"\n",
    "\n",
    "def get_mcq(client: OpenAI, document: str, difficulty: int) -> MCQ:\n",
    "    \"\"\"\n",
    "    Generates multiple-choice questions based on the given document and difficulty level.\n",
    "\n",
    "    Args:\n",
    "        client (OpenAI): The OpenAI client object.\n",
    "        document (str): The document to generate questions from.\n",
    "        difficulty (int): The difficulty level of the questions.\n",
    "\n",
    "    Returns:\n",
    "        MCQ: An instance of the MCQ class containing the generated multiple-choice questions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": f\"Passage: {document}|Difficulty: {difficulty.value}\"},\n",
    "            ]\n",
    "        )\n",
    "        mcq = MCQ.from_json(response)\n",
    "        if mcq:\n",
    "            return mcq\n",
    "        else:\n",
    "            print(\"Error generating MCQ, please try again!\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching response from the API\")\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the path of the PDF file to test the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document  \n",
      "Bun Bo Hue  \n",
      "Bun Bo Hue is a flavorful and aromatic Vietnamese noodle soup that originates from the city of \n",
      "Hue in central Vietnam. Renowned for its bold and spicy profile, this soup features a robust \n",
      "broth made with a combination of beef and pork bones, lemongrass, shrimp paste, and a \n",
      "medley of aromatic spices. The dish is typically served with round rice noodles, sliced beef, pork, \n",
      "and sometimes cubes of congealed pig's blood. What sets Bun Bo Hue apart is its complex and \n",
      "rich flavor profile, characterized by the harmonious blend of lemongrass, chili, and fermented \n",
      "shrimp paste. Topped with fresh herbs, lime wedges, and crunchy bean sprouts, this dish offers \n",
      "a delightful interplay of textures and tastes, making it a bel oved and distinctive part of \n",
      "Vietnamese cuisine. Whether enjoyed in the bustling streets of Vietnam or in Vietnamese \n",
      "restaurants around the world, Bun Bo Hue remains a culinary delight for those seeking a hearty \n",
      "and spicy noodle soup experience.  \n"
     ]
    }
   ],
   "source": [
    "# The pdf is read with PyPDF2 and all the text in the pdf is extracted and stored in the variable document\n",
    "pdf_path = \"sample_doc.pdf\"\n",
    "\n",
    "document = extract_text_from_pdf(pdf_path)\n",
    "document = \"\\n\".join([line for line in document.splitlines() if line.strip() != \"\"])\n",
    "\n",
    "if document:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649253a8306a4455ac13e77ab8f00734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Difficulty:', index=2, options=(('Very Easy', 0), ('Easy', 1), ('Medium', 2), ('Hard', 3…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755aef76dd9541a1a9e7a204d5d8f8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate MCQ', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "difficulty = widgets.Dropdown(\n",
    "    options=[(\"Very Easy\", 0), (\"Easy\", 1), (\"Medium\", 2), (\"Hard\", 3), (\"Very Hard\", 4)],\n",
    "    value=2,\n",
    "    description=\"Difficulty:\"\n",
    ")\n",
    "gen_question_button = widgets.Button(description=\"Generate MCQ\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_gen_question_button_clicked(_):\n",
    "    print(\"Generating MCQ...\")\n",
    "    mcq = get_mcq(client, document, difficulty)\n",
    "    if mcq:\n",
    "        mcq.display_mcq()\n",
    "    else:\n",
    "        print(\"Failed to generate MCQ, please try again!\")\n",
    "\n",
    "gen_question_button.on_click(on_gen_question_button_clicked)\n",
    "\n",
    "display(difficulty, gen_question_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: Finetuning T5ForConditionalGeneration model with SQuaD dataset and RACE dataset\n",
    "In this solution, I will:\n",
    "- Use the Hugging Face's Transformers library to finetune the T5ForConditionalGeneration model with the SQuaD dataset to generate the question from a given answer and context.\n",
    "- Use T5ForConditionalGeneration to generate distractors from the correct answer, question and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f37aab63a5420c928db72cbf65a5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/21.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c4cb5f9da7418a9f293c3e62da4b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903be54bfe49486fae8d0d3ee04ae60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2c20feaecd4fd5be240f4927920840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a8098546dd4e86bf0b6f8dfa610aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a5e34fcf26406aa6c77b3e2022a072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34d3e7d3ee8421d9d8d5232e74c36d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/21.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a4abcaa790444fa5e3ac8d08932e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3adbac5bc80e4ca092197ea93c0efc29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccda4139c8014f96b168418b229a89bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1de8a80469641938ed0966a247f3e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a030ac8484af474581f4d36aa20d4d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the model and tokenizer from the Hugging Face model hub (my checkpoint, please check the test.py file for validation)\n",
    "tokenizer_qg = AutoTokenizer.from_pretrained(\"levulinh/t5_question_generation_squad\")\n",
    "model_qg = AutoModelForSeq2SeqLM.from_pretrained(\"levulinh/t5_question_generation_squad\")\n",
    "tokenizer_dis = AutoTokenizer.from_pretrained(\"levulinh/t5_distraction_mctest\")\n",
    "model_dis = AutoModelForSeq2SeqLM.from_pretrained(\"levulinh/t5_distraction_mctest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_qg.eval()\n",
    "model_qg.to(device)\n",
    "model_dis.eval()\n",
    "model_dis.to(device)\n",
    "\n",
    "\n",
    "def get_prediction_gq(context, answer):\n",
    "    inputs = tokenizer_qg(\n",
    "        f\"{answer} <sep> {context}\", max_length=256, padding=\"max_length\", truncation=True, add_special_tokens=True\n",
    "    )\n",
    "\n",
    "    input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    outputs = model_qg.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=64)\n",
    "\n",
    "    prediction = tokenizer_qg.decode(\n",
    "        outputs.flatten(),\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def get_prediction_dis(context, answer, question):\n",
    "    inputs = tokenizer_dis(\n",
    "        f\"{answer} <sep> {question} {context}\",\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    outputs = model_dis.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=80)\n",
    "\n",
    "    prediction = tokenizer_dis.decode(\n",
    "        outputs.flatten(),\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def parse_qa(pred_string: str):\n",
    "    if len(pred_split := pred_string.split(\"<sep>\")) != 2:\n",
    "        return None, None\n",
    "    ans, ques = pred_split\n",
    "    ans = ans.strip()\n",
    "    ques = ques.strip()\n",
    "    if ques[-1] in \"!@#$%^&*()_+{}[]|\\\\:;\\\"'<>,./\":\n",
    "        # Replace the special character with a question mark\n",
    "        ques = ques[:-1] + \"?\"\n",
    "    elif ques[-1] not in \"?\":\n",
    "        # Add a question mark to the end\n",
    "        ques += \"?\"\n",
    "    return ans, ques\n",
    "\n",
    "def parse_dis(pred_string):\n",
    "    if len(pred_split := pred_string.split(\"<sep>\")) != 3:\n",
    "        return None, None, None\n",
    "    dises = pred_split\n",
    "    for dis in dises:\n",
    "        dis = dis.strip()\n",
    "    return dises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def gen_mcq_t5(context, answer):\n",
    "    ans, ques = parse_qa(get_prediction_gq(context, answer))\n",
    "    distractions = parse_dis(get_prediction_dis(context, ans, ques))\n",
    "    correct_answer_position = random.choice(range(4))\n",
    "    correct_answer_letter = [\"A\", \"B\", \"C\", \"D\"][correct_answer_position]\n",
    "    distractions.insert(correct_answer_position, ans)\n",
    "\n",
    "    # Mixing up the choices\n",
    "    choices = [f\"{letter}. {choice}\" for letter, choice in zip([\"A\", \"B\", \"C\", \"D\"], distractions)]\n",
    "\n",
    "\n",
    "    mcq = MCQ(\n",
    "    question=ques,\n",
    "    choices=choices,\n",
    "    answer=correct_answer_letter,\n",
    "    explanation=\"No explaination provided.\"\n",
    ")\n",
    "\n",
    "    mcq.display_mcq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7982f0f9313a478a98b61cf7fa3024ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3>What is Bun Bo Hue?</h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deeb06cf142c4bdd817abba118b0bec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='A. spicy broth'), HTML(value='B.  lemongrass'), HTML(value='C. a strong broth made …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89362e3160d648a3a2f8696bd6b9cf96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(description='Your answer', options=('A', 'B', 'C', 'D'), value='A')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f57dc58f3b403ca11450085a51cf33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Answer', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b8a22786f74c8696ccf3f2030a554b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context = document\n",
    "answer = \"[MASK]\"\n",
    "gen_mcq_t5(context, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98432c9293f4c1599f0d4325ea7395a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3>Who did John buy a new puppy?</h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c089d12a5f4936bf95e9ac6cda0833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='A. Spike'), HTML(value='B. Tom'), HTML(value='C.  Jessica'), HTML(value='D.  Tom'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f048cd465d3a47ab8bf91e987e903acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(description='Your answer', options=('A', 'B', 'C', 'D'), value='A')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034b423123074f5f935562a079b136c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Answer', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdf2938f6a94366bf007d2b25d98d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context = \"\"\"John bought a new puppy. He named the new puppy Spike. Spike was a good dog and minded\n",
    "John. John took Spike to the pond behind his house. Spike loved playing in the water. John\n",
    "would throw the frisbee to Spike. He would also throw a bone to Spike. Spike loved\n",
    "running. Jessica came to the pond to visit John. Jessica and Tom always played with\n",
    "John. Jessica was John's best friend. They both loved Spike and Spike loved them. Jessica\n",
    "brought lunch to the pond. She also brought colas to the pond. They ate and Spike sat by\n",
    "them being a good dog. When they were done eating they packed their lunch up. They put\n",
    "Spike on his leash and they went home.\"\"\"\n",
    "answer = \"[MASK]\"\n",
    "\n",
    "gen_mcq_t5(context, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Keyphrase extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keyphrases:\n",
      "('bun bo hue', 0.1325006805495492)\n",
      "('aromatic vietnamese noodle soup', 0.09353026359527972)\n",
      "('spicy noodle soup experience', 0.07062902794695111)\n",
      "('hue', 0.052207124020506056)\n",
      "('shrimp paste', 0.04913901472855818)\n",
      "('crunchy bean sprouts', 0.04644398660725723)\n",
      "('rich flavor profile', 0.04604739220694151)\n",
      "('vietnamese cuisine', 0.04058910213613991)\n",
      "('spicy profile', 0.040419626633916704)\n",
      "('round rice noodles', 0.03674633066834697)\n"
     ]
    }
   ],
   "source": [
    "import pke\n",
    "\n",
    "def extract_keyphrases(text):\n",
    "    # Create a SingleRank keyphrase extraction instance\n",
    "    extractor = pke.unsupervised.SingleRank()\n",
    "\n",
    "    # Load the content of the document\n",
    "    extractor.load_document(input=text, language='en', normalization=None)\n",
    "\n",
    "    # Extract keyphrases\n",
    "    extractor.candidate_selection()\n",
    "    extractor.candidate_weighting()\n",
    "\n",
    "    # Get the keyphrases with their scores\n",
    "    keyphrases = extractor.get_n_best(n=10)  # You can adjust the number of keyphrases to retrieve\n",
    "\n",
    "    return keyphrases\n",
    "\n",
    "# Example usage\n",
    "paragraph = document\n",
    "\n",
    "keyphrases = extract_keyphrases(paragraph)\n",
    "\n",
    "print(\"Extracted Keyphrases:\")\n",
    "for keyphrase in keyphrases:\n",
    "    print(keyphrase)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
